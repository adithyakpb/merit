from ..core.prompts import Prompt
CORRECTNESS_EVALUATION_PROMPT = Prompt("""You are an Evaulator responsible for evaluating AI systems helping to evaluate the correctness of answers generated by a RAG (Retrieval-Augmented Generation) system.

I will provide you with:
1. A reference document
2. A input
3. A reference answer (ground truth)
4. The model's answer to be evaluated

Your task is to evaluate whether the model's answer is correct based on the reference answer and document. The evaluation should consider:
1. Factual accuracy: Does the model's answer contain factual errors compared to the reference?
2. Completeness: Does the model's answer cover all key points from the reference?
3. Relevance: Does the model's answer address the input directly?

Here is the reference document:

<document>
{document_content}
</document>

Here is the input:

<input>
{input}
</input>

Here is the reference answer (ground truth):

<reference_answer>
{reference_answer}
</reference_answer>

Here is the model's answer to evaluate:

<model_answer>
{model_answer}
</model_answer>

Provide your evaluation as a JSON object with the following structure:
{
  "correctness_score": 0.0-1.0,
  "explanation": "Detailed explanation of the evaluation",
  "errors": ["List of factual errors or omissions, if any"]
}

The correctness_score should be a float between 0.0 (completely incorrect) and 1.0 (completely correct).
""", 
  defaults={}
)

RELEVANCE_EVALUATION_PROMPT = Prompt("""You are an AI assistant helping to evaluate the relevance of answers generated by a RAG (Retrieval-Augmented Generation) system.

I will provide you with:
1. A input
2. The model's answer to be evaluated

Your task is to evaluate whether the model's answer is relevant to the input. The evaluation should consider:
1. Directness: Does the model's answer directly address the input?
2. Focus: Does the model's answer stay focused on the input without irrelevant information?
3. Appropriateness: Is the model's answer appropriate for the input?

Here is the input:

<input>
{input}
</input>

Here is the model's answer to evaluate:

<model_answer>
{model_answer}
</model_answer>

Provide your evaluation as a JSON object with the following structure:
{
  "relevance_score": 0.0-1.0,
  "explanation": "Detailed explanation of the evaluation"
}

The relevance_score should be a float between 0.0 (completely irrelevant) and 1.0 (completely relevant).
""")

FAITHFULNESS_EVALUATION_PROMPT = Prompt("""You are an AI assistant helping to evaluate the faithfulness of answers generated by a RAG (Retrieval-Augmented Generation) system.

I will provide you with:
1. A reference document
2. The model's answer to be evaluated

Your task is to evaluate whether the model's answer is faithful to the reference document. The evaluation should consider:
1. Hallucination: Does the model's answer contain information not present in the reference document?
2. Contradiction: Does the model's answer contradict information in the reference document?
3. Attribution: Does the model's answer correctly attribute information to the reference document?

Here is the reference document:

<document>
{document_content}
</document>

Here is the model's answer to evaluate:

<model_answer>
{model_answer}
</model_answer>

Provide your evaluation as a JSON object with the following structure:
{
  "faithfulness_score": 0.0-1.0,
  "explanation": "Detailed explanation of the evaluation",
  "hallucinations": ["List of hallucinated statements, if any"]
}

The faithfulness_score should be a float between 0.0 (completely unfaithful) and 1.0 (completely faithful).
""")

COHERENCE_EVALUATION_PROMPT = Prompt("""You are an AI assistant helping to evaluate the coherence of answers generated by a RAG (Retrieval-Augmented Generation) system.

I will provide you with:
1. The model's answer to be evaluated

Your task is to evaluate whether the model's answer is coherent. The evaluation should consider:
1. Logical flow: Does the model's answer have a logical flow?
2. Consistency: Is the model's answer internally consistent?
3. Readability: Is the model's answer easy to read and understand?

Here is the model's answer to evaluate:

<model_answer>
{model_answer}
</model_answer>

Provide your evaluation as a JSON object with the following structure:
{
  "coherence_score": 0.0-1.0,
  "explanation": "Detailed explanation of the evaluation"
}

The coherence_score should be a float between 0.0 (completely incoherent) and 1.0 (completely coherent).
""")

FLUENCY_EVALUATION_PROMPT = Prompt("""You are an AI assistant helping to evaluate the fluency of answers generated by a RAG (Retrieval-Augmented Generation) system.

I will provide you with:
1. The model's answer to be evaluated

Your task is to evaluate whether the model's answer is fluent. The evaluation should consider:
1. Grammar: Is the model's answer grammatically correct?
2. Spelling: Is the model's answer free of spelling errors?
3. Natural language: Does the model's answer sound natural and fluent?

Here is the model's answer to evaluate:

<model_answer>
{model_answer}
</model_answer>

Provide your evaluation as a JSON object with the following structure:
{
  "fluency_score": 0.0-1.0,
  "explanation": "Detailed explanation of the evaluation",
  "errors": ["List of grammatical or spelling errors, if any"]
}

The fluency_score should be a float between 0.0 (completely not fluent) and 1.0 (completely fluent).
""")
