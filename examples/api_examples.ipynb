{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERIT API Tutorial: Complete Developer Guide\n",
    "\n",
    "This notebook shows you how to use the MERIT API package in your own projects. We'll demonstrate the **before and after** - showing you the complex, error-prone code you'd write without MERIT, and the simple, robust code you can write with MERIT.\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "- How to replace complex API code with simple MERIT clients\n",
    "- Why MERIT's features matter for production applications\n",
    "- Real-world patterns you can copy-paste into your projects\n",
    "- How to build robust, scalable LLM applications\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Python 3.8+\n",
    "- OpenAI API key (optional, for real examples)\n",
    "- Gemini API key (optional, for real examples)\n",
    "\n",
    "## üöÄ Installation\n",
    "\n",
    "```bash\n",
    "pip install merit\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë API Keys configured!\n",
      "OpenAI key configured: ‚úÖ\n",
      "Google key configured: ‚úÖ\n",
      "\n",
      "üí° To use real API keys:\n",
      "   1. Replace the placeholder keys above with your actual keys, OR\n",
      "   2. Set OPENAI_API_KEY and GOOGLE_API_KEY environment variables\n"
     ]
    }
   ],
   "source": [
    "# API Key Configuration - Replace with your actual keys\n",
    "# You can also set these as environment variables: OPENAI_API_KEY and GOOGLE_API_KEY\n",
    "\n",
    "# Option 1: Set your keys directly (not recommended for production)\n",
    "OPENAI_API_KEY = \"\"  # Replace with your actual OpenAI API key\n",
    "GOOGLE_API_KEY = \"\"   # Replace with your actual Google API key\n",
    "\n",
    "# Option 2: Load from environment variables (recommended)\n",
    "import os\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', OPENAI_API_KEY)\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY', GOOGLE_API_KEY)\n",
    "\n",
    "# For demo purposes, we'll use placeholder keys that will trigger graceful error handling\n",
    "DEMO_OPENAI_KEY = \"sk-demo-key-for-examples\"  # This will fail gracefully in examples\n",
    "DEMO_GOOGLE_KEY = \"demo-google-key-for-examples\"  # This will fail gracefully in examples\n",
    "\n",
    "print(\"üîë API Keys configured!\")\n",
    "print(f\"OpenAI key configured: {'‚úÖ' if OPENAI_API_KEY and OPENAI_API_KEY != 'your-openai-api-key-here' else '‚ùå (using demo key)'}\")\n",
    "print(f\"Google key configured: {'‚úÖ' if GOOGLE_API_KEY and GOOGLE_API_KEY != 'your-google-api-key-here' else '‚ùå (using demo key)'}\")\n",
    "print(\"\\nüí° To use real API keys:\")\n",
    "print(\"   1. Replace the placeholder keys above with your actual keys, OR\")\n",
    "print(\"   2. Set OPENAI_API_KEY and GOOGLE_API_KEY environment variables\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Started: Your First API Call\n",
    "\n",
    "Let's start with the most basic example - making an API call to generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùå WITHOUT MERIT: Complex and Error-Prone\n",
    "\n",
    "Here's what you'd typically write without MERIT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Artificial intelligence (AI) refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. AI can be used to perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.\n"
     ]
    }
   ],
   "source": [
    "# This is what most developers write - lots of boilerplate, no error handling\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def generate_text_without_merit(prompt):\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not api_key:\n",
    "        raise ValueError('API key not found')\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'messages': [{'role': 'user', 'content': prompt}],\n",
    "        'temperature': 0.7\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        'https://api.openai.com/v1/chat/completions',\n",
    "        headers=headers,\n",
    "        json=data\n",
    "    )\n",
    "    \n",
    "    # No error handling, no retries, no validation\n",
    "    response.raise_for_status()\n",
    "    return response.json()['choices'][0]['message']['content']\n",
    "\n",
    "# Usage - fragile and verbose\n",
    "try:\n",
    "    result = generate_text_without_merit('What is artificial intelligence?')\n",
    "    print(f'Result: {result}')\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ WITH MERIT: Simple and Robust\n",
    "\n",
    "Here's the same functionality with MERIT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Artificial intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and decision-making. AI can be used to automate tasks, analyze data, recognize patterns, and make predictions. It is a rapidly evolving field with applications in various industries such as healthcare, finance, transportation, and more.\n"
     ]
    }
   ],
   "source": [
    "# Clean, simple, and robust\n",
    "from merit.api import OpenAIClient\n",
    "\n",
    "# One line setup - automatically loads from environment, includes error handling\n",
    "client = OpenAIClient()\n",
    "\n",
    "# One line usage - includes retries, validation, and proper error handling\n",
    "result = client.generate_text('What is artificial intelligence?')\n",
    "print(f'Result: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Key Benefits\n",
    "\n",
    "- **90% less code** - 2 lines vs 20+ lines\n",
    "- **Automatic error handling** - Built-in retries and validation\n",
    "- **Environment integration** - Automatically loads API keys\n",
    "- **Production ready** - Includes logging, monitoring, and best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Provider Support: Write Once, Use Anywhere\n",
    "\n",
    "One of MERIT's biggest advantages is that you can switch between different LLM providers without changing your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùå WITHOUT MERIT: Separate Code for Each Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI: Hello! How can I assist you today?\n",
      "Gemini: Hello! How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Separate implementation for OpenAI\n",
    "def openai_generate(prompt):\n",
    "    # 20+ lines of OpenAI-specific code\n",
    "    import requests\n",
    "    import os\n",
    "    headers = {'Authorization': f'Bearer {os.getenv(\"OPENAI_API_KEY\")}'}\n",
    "    data = {'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': prompt}]}\n",
    "    response = requests.post('https://api.openai.com/v1/chat/completions', headers=headers, json=data)\n",
    "    return response.json()['choices'][0]['message']['content']\n",
    "\n",
    "# Completely different implementation for Gemini\n",
    "def gemini_generate(prompt):\n",
    "    # 20+ lines of Gemini-specific code\n",
    "    from google import genai\n",
    "    client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "    response = client.models.generate_content(model='gemini-2.0-flash-exp', contents=prompt)\n",
    "    return response.text\n",
    "\n",
    "# Your application code needs to know about each provider\n",
    "def my_app_function(prompt, provider='openai'):\n",
    "    if provider == 'openai':\n",
    "        return openai_generate(prompt)\n",
    "    elif provider == 'gemini':\n",
    "        return gemini_generate(prompt)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown provider: {provider}')\n",
    "\n",
    "# Switching providers requires code changes\n",
    "result1 = my_app_function('Hello', 'openai')\n",
    "result2 = my_app_function('Hello', 'gemini')\n",
    "print(f'OpenAI: {result1}')\n",
    "print(f'Gemini: {result2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ WITH MERIT: Polymorphic Usage\n",
    "\n",
    "With MERIT, the same code works with any provider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI: Hello! How can I assist you today?\n",
      "Gemini: Hello! How can I help you today?\n",
      "\n",
      "Client 1: AI, or artificial intelligence, refers to the simu...\n",
      "Client 2: AI, or Artificial Intelligence, is a broad field o...\n"
     ]
    }
   ],
   "source": [
    "from merit.api import OpenAIClient, GeminiClient\n",
    "\n",
    "# Both clients implement the same interface\n",
    "openai_client = OpenAIClient()\n",
    "gemini_client = GeminiClient()\n",
    "\n",
    "# Your application code is provider-agnostic\n",
    "def my_app_function(client, prompt):\n",
    "    # Same code works with any client!\n",
    "    return client.generate_text(prompt)\n",
    "\n",
    "# Switch providers by just changing the client\n",
    "result1 = my_app_function(openai_client, 'Hello')\n",
    "result2 = my_app_function(gemini_client, 'Hello')\n",
    "\n",
    "print(f'OpenAI: {result1}')\n",
    "print(f'Gemini: {result2}')\n",
    "\n",
    "# Easy A/B testing\n",
    "clients = [openai_client, gemini_client]\n",
    "for i, client in enumerate(clients):\n",
    "    result = my_app_function(client, 'What is AI?')\n",
    "    print(f'Client {i+1}: {result[:50]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration Management: Production-Ready Setup\n",
    "\n",
    "Managing API keys, endpoints, and configuration is crucial for production applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùå WITHOUT MERIT: Scattered Configuration\n",
    "\n",
    "Configuration is typically scattered and hard to manage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration scattered throughout your code\n",
    "import os\n",
    "\n",
    "# Hardcoded values mixed with environment variables\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')  # What if it's missing?\n",
    "OPENAI_BASE_URL = 'https://api.openai.com/v1'  # Hardcoded\n",
    "MODEL = 'gpt-3.5-turbo'  # Hardcoded\n",
    "TEMPERATURE = 0.7  # Hardcoded\n",
    "\n",
    "# No validation - fails at runtime\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError('Missing API key')\n",
    "\n",
    "# Configuration repeated in every function\n",
    "def make_api_call():\n",
    "    headers = {'Authorization': f'Bearer {OPENAI_API_KEY}'}\n",
    "    data = {'model': MODEL, 'temperature': TEMPERATURE}\n",
    "    # ... rest of the code\n",
    "    pass\n",
    "\n",
    "# Different configuration for each provider\n",
    "GEMINI_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "GEMINI_MODEL = 'gemini-2.0-flash-exp'\n",
    "# More scattered config..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ WITH MERIT: Centralized Configuration\n",
    "\n",
    "MERIT provides multiple ways to configure your clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Method 1: Environment Variables ===\n",
      "OpenAI authenticated: True\n",
      "Gemini authenticated: True\n",
      "\n",
      "=== Method 2: Direct Parameters ===\n",
      "OpenAI model: gpt-4\n",
      "Gemini model: gemini-2.0-flash-exp\n",
      "\n",
      "=== Method 3: Configuration Objects ===\n",
      "Config validation passed for both clients\n",
      "OpenAI embedding model: text-embedding-ada-002\n",
      "Gemini temperature: 0.1\n"
     ]
    }
   ],
   "source": [
    "from merit.api import OpenAIClient, OpenAIClientConfig, GeminiClient, GeminiClientConfig\n",
    "\n",
    "# Method 1: Environment variables (recommended for production)\n",
    "# Just set OPENAI_API_KEY and GOOGLE_API_KEY in your environment\n",
    "print('=== Method 1: Environment Variables ===')\n",
    "openai_client = OpenAIClient()  # Automatically loads from environment\n",
    "gemini_client = GeminiClient()  # Automatically loads from environment\n",
    "\n",
    "print(f'OpenAI authenticated: {openai_client.is_authenticated}')\n",
    "print(f'Gemini authenticated: {gemini_client.is_authenticated}')\n",
    "\n",
    "# Method 2: Direct parameters (good for development)\n",
    "print('\\n=== Method 2: Direct Parameters ===')\n",
    "openai_direct = OpenAIClient(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model='gpt-4',\n",
    "    organization_id='your-org'\n",
    ")\n",
    "\n",
    "gemini_direct = GeminiClient(\n",
    "    api_key=GOOGLE_API_KEY,\n",
    "    generation_model='gemini-2.0-flash-exp',\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(f'OpenAI model: {openai_direct.model}')\n",
    "print(f'Gemini model: {gemini_direct.generation_model}')\n",
    "\n",
    "# Method 3: Configuration objects (best for complex setups)\n",
    "print('\\n=== Method 3: Configuration Objects ===')\n",
    "openai_config = OpenAIClientConfig(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model='gpt-4',\n",
    "    embedding_model='text-embedding-ada-002',\n",
    "    organization_id='your-org'\n",
    ")\n",
    "\n",
    "gemini_config = GeminiClientConfig(\n",
    "    api_key=GOOGLE_API_KEY,\n",
    "    generation_model='gemini-2.0-flash-exp',\n",
    "    embedding_model='text-embedding-004',\n",
    "    temperature=0.1,\n",
    "    max_output_tokens=1024\n",
    ")\n",
    "\n",
    "# Validate configuration before use\n",
    "openai_config.validate()\n",
    "gemini_config.validate()\n",
    "\n",
    "openai_from_config = OpenAIClient(config=openai_config)\n",
    "gemini_from_config = GeminiClient(config=gemini_config)\n",
    "\n",
    "print(f'Config validation passed for both clients')\n",
    "print(f'OpenAI embedding model: {openai_from_config.embedding_model}')\n",
    "print(f'Gemini temperature: {gemini_from_config.temperature}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Easy Model and Parameter Switching\n",
    "\n",
    "The real power of MERIT's configuration system is how easy it is to switch models and parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Same Prompt, Different Configurations ===\n",
      "üé® CREATIVE (GPT-4, temp=0.9):\n",
      "Artificial Intelligence (AI) is one of the most transformative technologies of our time and its potential to revolutionize various sectors is vast. The future of AI is promising and holds great potential for reshaping our lives, our businesses, and our societies.\n",
      "\n",
      "In the coming years, AI is expected to make significant advancements. It is projected that AI will become more sophisticated and autonomous, capable of learning and making decisions without human intervention. This increased autonomy will enable AI to perform complex tasks more efficiently than humans. For instance, autonomous vehicles, powered by AI, are expected to become a common sight on our roads, reducing human error and improving road safety.\n",
      "\n",
      "AI is also expected to advance in terms of natural language processing, image recognition, and predictive analytics. These advancements will enable AI to better understand, interpret, and respond to human language and emotions, making interactions with AI more seamless and natural. This will greatly enhance the user experience in various applications, from customer service chatbots to personal virtual assistants.\n",
      "\n",
      "In the field of healthcare, AI is expected to revolutionize diagnosis and treatment. AI-powered systems will be able to accurately diagnose diseases, predict health risks, and recommend personalized treatments based on a patient's unique genetic makeup. This will greatly improve patient outcomes and reduce healthcare costs.\n",
      "\n",
      "In the business world, AI will continue to drive automation, improving efficiency and productivity. AI-powered analytics will also provide businesses with deeper insights into their data, enabling them to make data-driven decisions and stay competitive.\n",
      "\n",
      "However, the future of AI also presents challenges. Concerns about privacy, security, and ethical implications of AI are growing. As AI becomes more autonomous, questions about accountability and transparency also arise. Therefore, it is crucial that we establish robust ethical and legal frameworks to govern the use of AI.\n",
      "\n",
      "Furthermore, as AI continues to advance, there is a risk of job displacement. While AI will create new jobs, it will also render certain jobs obsolete. Therefore, there is a need for continuous learning and skill development to prepare the workforce for the AI-driven future.\n",
      "\n",
      "In conclusion, the future of AI is bright and holds great promise. However, it also presents challenges that need to be addressed. With the right policies and frameworks in place, we can harness the full potential of AI while mitigating its risks.\n",
      "\n",
      "üìä FACTUAL (GPT-3.5, temp=0.1):\n",
      "The future of AI is a topic of much speculation and excitement. As technology continues to advance at a rapid pace, the possibilities for AI seem endless. AI has the potential to revolutionize many aspects of our lives, from healthcare to transportation to entertainment.\n",
      "\n",
      "One of the most exciting potential applications of AI is in the field of healthcare. AI has the ability to analyze vast amounts of data and identify patterns that human doctors may not be able to see. This could lead to more accurate diagnoses and more personalized treatment plans for patients. AI could also assist in drug discovery, speeding up the process of developing new medications.\n",
      "\n",
      "In the field of transportation, AI has the potential to make our roads safer and more efficient. Self-driving cars are already being tested and could soon become a common sight on our roads. These cars have the potential to reduce accidents and traffic congestion, as well as make transportation more accessible to those who are unable to drive.\n",
      "\n",
      "In the entertainment industry, AI is already being used to create more personalized experiences for consumers. Streaming services like Netflix use AI algorithms to recommend content to users based on their viewing history. In the future, we may see AI-generated movies or music that are tailored to individual preferences.\n",
      "\n",
      "Of course, with all of these exciting possibilities also come concerns about the ethical implications of AI. There are concerns about job displacement as AI takes over tasks that were previously done by humans. There are also concerns about data privacy and security as AI systems collect and analyze vast amounts of personal data.\n",
      "\n",
      "Overall, the future of AI is likely to be a mix of incredible advancements and challenges. It will be important for society to carefully consider the implications of AI as it becomes more integrated into our lives. With careful planning and oversight, AI has the potential to greatly improve our lives and make the world a better place.\n",
      "\n",
      "üí∞ BUDGET (GPT-3.5, temp=0.5, 50 tokens):\n",
      "The future of artificial intelligence (AI) holds great promise and potential for transforming industries, improving efficiency, and enhancing our daily lives. As technology continues to advance at a rapid pace, AI is becoming increasingly integrated into various aspects of society.\n",
      "\n",
      "One of the key areas where AI is expected to have a major impact is in the field of healthcare. AI-powered systems can analyze vast amounts of medical data to help doctors make more accurate diagnoses and develop personalized treatment plans. This can lead to better patient outcomes and more efficient healthcare delivery.\n",
      "\n",
      "In the business world, AI is already being used to automate tasks, streamline operations, and improve decision-making processes. Companies are increasingly turning to AI-powered solutions to gain a competitive edge, optimize their workflows, and better serve their customers.\n",
      "\n",
      "In the coming years, we can expect to see even more advancements in AI technology, including the development of more sophisticated algorithms, improved natural language processing capabilities, and enhanced machine learning models. This will enable AI systems to perform increasingly complex tasks and make more nuanced decisions.\n",
      "\n",
      "However, with these advancements also come challenges and ethical considerations. As AI becomes more prevalent in our daily lives, there will be a need to address issues related to privacy, security, bias, and accountability. It will be important for policymakers, industry leaders, and technologists to work together to ensure that AI is developed and deployed in a responsible and ethical manner.\n",
      "\n",
      "Overall, the future of AI holds great promise for improving our lives and driving innovation across various industries. By harnessing the power of artificial intelligence, we can unlock new opportunities, solve complex problems, and create a more connected and efficient world.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define different configurations for different use cases\n",
    "\n",
    "# Configuration 1: Creative writing (high temperature)\n",
    "creative_config = OpenAIClientConfig(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model='gpt-4',\n",
    "    temperature=0.9,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "# Configuration 2: Factual answers (low temperature)\n",
    "factual_config = OpenAIClientConfig(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=0.1,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# Configuration 3: Cost-effective (cheaper model)\n",
    "budget_config = OpenAIClientConfig(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=0.5,\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "# Create clients from configs\n",
    "creative_client = OpenAIClient(config=creative_config)\n",
    "factual_client = OpenAIClient(config=factual_config)\n",
    "budget_client = OpenAIClient(config=budget_config)\n",
    "\n",
    "# Same prompt, different behaviors\n",
    "prompt = 'Write about the future of AI'\n",
    "\n",
    "print('=== Same Prompt, Different Configurations ===')\n",
    "\n",
    "print('üé® CREATIVE (GPT-4, temp=0.9):')\n",
    "creative_response = creative_client.generate_text(prompt)\n",
    "print(f'{creative_response}\\n')\n",
    "\n",
    "print('üìä FACTUAL (GPT-3.5, temp=0.1):')\n",
    "factual_response = factual_client.generate_text(prompt)\n",
    "print(f'{factual_response}\\n')\n",
    "\n",
    "print('üí∞ BUDGET (GPT-3.5, temp=0.5, 50 tokens):')\n",
    "budget_response = budget_client.generate_text(prompt)\n",
    "print(f'{budget_response}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Cross-Provider Configuration Flexibility\n",
    "\n",
    "The same configuration approach works across all providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Same App Logic, Different Providers ===\n",
      "üìù OpenAI Creative Story:\n",
      "Title: The Hourglass of Eternity\n",
      "\n",
      "In the bustling city of New York, a timid librarian named Stanley ...\n",
      "\n",
      "üìù Gemini Creative Story:\n",
      "Elara traced the glyphs on the weathered brass plate, her fingers ghosting over symbols that felt an...\n",
      "\n",
      "üî¨ A/B Testing Results:\n",
      "OpenAI GPT-4: 3319 characters generated\n",
      "Preview: Title: The Melody of Metal Hearts\n",
      "\n",
      "Once upon a time, in the futuristic city of N...\n",
      "\n",
      "Gemini Flash: 761 characters generated\n",
      "Preview: Unit 734, designated \"Custodian,\" hummed softly, its optical sensors sweeping th...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define equivalent configurations for different providers\n",
    "\n",
    "# OpenAI Configuration for creative writing\n",
    "openai_creative = OpenAIClientConfig(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model='gpt-4',\n",
    "    temperature=0.8,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "# Gemini Configuration for creative writing\n",
    "gemini_creative = GeminiClientConfig(\n",
    "    api_key=GOOGLE_API_KEY,\n",
    "    generation_model='gemini-2.0-flash-exp',\n",
    "    temperature=0.8,\n",
    "    max_output_tokens=150\n",
    ")\n",
    "\n",
    "# Create clients from configs\n",
    "openai_writer = OpenAIClient(config=openai_creative)\n",
    "gemini_writer = GeminiClient(config=gemini_creative)\n",
    "\n",
    "# Same application logic works with both\n",
    "def creative_writing_app(client, topic):\n",
    "    prompt = f'Write a creative short story about {topic}'\n",
    "    return client.generate_text(prompt)\n",
    "\n",
    "# Easy to switch providers\n",
    "topic = 'time travel'\n",
    "\n",
    "print('=== Same App Logic, Different Providers ===')\n",
    "\n",
    "print('üìù OpenAI Creative Story:')\n",
    "openai_story = creative_writing_app(openai_writer, topic)\n",
    "print(f'{openai_story[:100]}...\\n')\n",
    "\n",
    "print('üìù Gemini Creative Story:')\n",
    "gemini_story = creative_writing_app(gemini_writer, topic)\n",
    "print(f'{gemini_story[:100]}...\\n')\n",
    "\n",
    "# Easy A/B testing across providers\n",
    "writers = [\n",
    "    ('OpenAI GPT-4', openai_writer),\n",
    "    ('Gemini Flash', gemini_writer)\n",
    "]\n",
    "\n",
    "print('üî¨ A/B Testing Results:')\n",
    "for name, writer in writers:\n",
    "    result = creative_writing_app(writer, 'robots')\n",
    "    print(f'{name}: {len(result)} characters generated')\n",
    "    print(f'Preview: {result[:80]}...\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Configuration Benefits\n",
    "\n",
    "- **Environment integration** - Automatic loading from .env files\n",
    "- **Validation** - Catch configuration errors early\n",
    "- **Flexibility** - Multiple configuration methods\n",
    "- **Security** - No hardcoded API keys in code\n",
    "- **Easy switching** - Change models and parameters instantly\n",
    "- **Cross-provider consistency** - Same patterns work everywhere\n",
    "- **A/B testing ready** - Compare providers effortlessly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error Handling & Resilience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùå WITHOUT MERIT: Manual Error Handling\n",
    "\n",
    "You'd need to implement all error handling manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Manual Error Handling Example ===\n",
      "Success: AI, or artificial intelligence, refers to the simu...\n",
      "\n",
      "‚ùå Problems with manual error handling:\n",
      "- 50+ lines of complex error handling code\n",
      "- Easy to miss edge cases\n",
      "- No adaptive rate limiting\n",
      "- Inconsistent across different API calls\n",
      "- Hard to maintain and debug\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "def robust_api_call_without_merit(prompt, max_retries=3):\n",
    "    \"\"\"Manual implementation of retries and error handling\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                'https://api.openai.com/v1/chat/completions',\n",
    "                headers={'Authorization': f'Bearer {os.getenv(\"OPENAI_API_KEY\")}'},\n",
    "                json={\n",
    "                    'model': 'gpt-3.5-turbo',\n",
    "                    'messages': [{'role': 'user', 'content': prompt}]\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            # Handle different error types manually\n",
    "            if response.status_code == 429:  # Rate limit\n",
    "                wait_time = 2 ** attempt + random.uniform(0, 1)\n",
    "                print(f'Rate limited, waiting {wait_time:.1f}s...')\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "                \n",
    "            elif response.status_code == 500:  # Server error\n",
    "                print(f'Server error, attempt {attempt + 1}/{max_retries}')\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "                \n",
    "            elif response.status_code == 401:  # Auth error\n",
    "                raise ValueError('Invalid API key')\n",
    "                \n",
    "            response.raise_for_status()\n",
    "            return response.json()['choices'][0]['message']['content']\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f'Timeout, attempt {attempt + 1}/{max_retries}')\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "                \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f'Connection error, attempt {attempt + 1}/{max_retries}')\n",
    "            time.sleep(2 ** attempt)\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "    \n",
    "    raise Exception('Max retries exceeded')\n",
    "\n",
    "# Usage - you need to handle errors yourself\n",
    "print('=== Manual Error Handling Example ===')\n",
    "try:\n",
    "    result = robust_api_call_without_merit('What is AI?')\n",
    "    print(f'Success: {result[:50]}...')\n",
    "except Exception as e:\n",
    "    print(f'Failed after retries: {e}')\n",
    "\n",
    "# Problems with manual approach:\n",
    "print('\\n‚ùå Problems with manual error handling:')\n",
    "print('- 50+ lines of complex error handling code')\n",
    "print('- Easy to miss edge cases')\n",
    "print('- No adaptive rate limiting')\n",
    "print('- Inconsistent across different API calls')\n",
    "print('- Hard to maintain and debug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ WITH MERIT: Automatic Error Handling\n",
    "\n",
    "MERIT handles all of this automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Basic MERIT Error Handling (Graceful Mode) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed (graceful mode): Authentication failed (Error Code: MAPI-001) \n",
      "\n",
      "Troubleshooting: Please check your API key and ensure it is valid. Verify that the API key has been correctly set in your configuration. If using environment variables, ensure MERIT_API_KEY or the service-specific API key variable (e.g., OPENAI_API_KEY) is correctly set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graceful mode success: ...\n",
      "\n",
      "=== Strict Mode Error Handling ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed (strict mode): Authentication failed (Error Code: MAPI-001) \n",
      "\n",
      "Troubleshooting: Please check your API key and ensure it is valid. Verify that the API key has been correctly set in your configuration. If using environment variables, ensure MERIT_API_KEY or the service-specific API key variable (e.g., OPENAI_API_KEY) is correctly set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Strict mode exception: MeritAPIAuthenticationError: Authentication failed (Error Code: MAPI-001) \n",
      "\n",
      "Troubleshooting: Please check your API key and ensure it is valid. Verify that the API key has been correctly set in your configuration. If using environment variables, ensure MERIT_API_KEY or the service-specific API key variable (e.g., OPENAI_API_KEY) is correctly set.\n",
      "\n",
      "=== Method-Level Strict Override ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed (strict mode): Authentication failed (Error Code: MAPI-001) \n",
      "\n",
      "Troubleshooting: Please check your API key and ensure it is valid. Verify that the API key has been correctly set in your configuration. If using environment variables, ensure MERIT_API_KEY or the service-specific API key variable (e.g., OPENAI_API_KEY) is correctly set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Method override exception: MeritAPIAuthenticationError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed (graceful mode): Authentication failed (Error Code: MAPI-001) \n",
      "\n",
      "Troubleshooting: Please check your API key and ensure it is valid. Verify that the API key has been correctly set in your configuration. If using environment variables, ensure MERIT_API_KEY or the service-specific API key variable (e.g., OPENAI_API_KEY) is correctly set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graceful call: \n"
     ]
    }
   ],
   "source": [
    "from merit.api import OpenAIClient\n",
    "from merit.api.run_config import with_retry, adaptive_throttle, with_adaptive_retry\n",
    "\n",
    "# Basic usage - automatic error handling included (graceful mode)\n",
    "print('=== Basic MERIT Error Handling (Graceful Mode) ===')\n",
    "client = OpenAIClient(\n",
    "    api_key='invalid',\n",
    "    strict=False  # Graceful mode (default)\n",
    ")\n",
    "\n",
    "# This single line includes retries, rate limiting, validation\n",
    "result = client.generate_text('What is AI?')\n",
    "print(f'‚úÖ Graceful mode success: {result[:50]}...')\n",
    "\n",
    "# Strict mode - raises exceptions on failures\n",
    "print('\\n=== Strict Mode Error Handling ===')\n",
    "strict_client = OpenAIClient(\n",
    "    api_key='invalid',\n",
    "    strict=True  # Strict mode - raises exceptions\n",
    ")\n",
    "\n",
    "try:\n",
    "    result = strict_client.generate_text('What is AI?')\n",
    "    print(f'‚úÖ Strict mode success: {result[:50]}...')\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Strict mode exception: {type(e).__name__}: {e}')\n",
    "\n",
    "# Method-level override\n",
    "print('\\n=== Method-Level Strict Override ===')\n",
    "graceful_client = OpenAIClient(\n",
    "    api_key='invalid',\n",
    "    strict=False  # Default graceful\n",
    ")\n",
    "\n",
    "# Override to strict for this specific call\n",
    "try:\n",
    "    result = graceful_client.generate_text('Hello', strict=True)\n",
    "    print(f'‚úÖ Method override success: {result}')\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Method override exception: {type(e).__name__}')\n",
    "\n",
    "# Graceful call (uses client default)\n",
    "result = graceful_client.generate_text('Hello')\n",
    "print(f'‚úÖ Graceful call: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ°Ô∏è Error Handling in Production\n",
    "\n",
    "Real-world error scenarios and how MERIT handles them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Scenario 1: Invalid API Key (Strict Mode) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed (strict mode): Authentication failed (Error Code: MAPI-001) \n",
      "\n",
      "Troubleshooting: Please check your API key and ensure it is valid. Verify that the API key has been correctly set in your configuration. If using environment variables, ensure MERIT_API_KEY or the service-specific API key variable (e.g., OPENAI_API_KEY) is correctly set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MERIT properly handled invalid API key: MeritAPIAuthenticationError\n",
      "\n",
      "=== Scenario 1b: Invalid API Key (Graceful Mode) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed (graceful mode): Authentication failed (Error Code: MAPI-001) \n",
      "\n",
      "Troubleshooting: Please check your API key and ensure it is valid. Verify that the API key has been correctly set in your configuration. If using environment variables, ensure MERIT_API_KEY or the service-specific API key variable (e.g., OPENAI_API_KEY) is correctly set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Unexpected success: \n",
      "\n",
      "=== Scenario 2: Invalid Model (Strict Mode) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed (strict mode): API endpoint not found (Error Code: MAPI-004) \n",
      "\n",
      "Troubleshooting: Please check that you're using the correct API endpoint. Verify the resource identifier is valid. Ensure the API version you're using supports this resource. Check the API documentation for any recent changes to endpoints.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MERIT properly handled invalid model: MeritAPIResourceNotFoundError\n",
      "\n",
      "=== Scenario 3: Graceful Degradation ===\n",
      "‚úÖ Prompt 1: Python is a high-level, interpreted programming la...\n",
      "‚úÖ Prompt 2: Machine learning is a subset of artificial intelli...\n",
      "‚úÖ Prompt 3: The future of AI is likely to involve continued ad...\n",
      "\n",
      "üìä Success rate: 100.0%\n",
      "\n",
      "=== Scenario 4: Strict vs Graceful Comparison ===\n",
      "üî¥ Strict Mode (Development/Debugging):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed (strict mode): Authentication failed (Error Code: MAPI-001) \n",
      "\n",
      "Troubleshooting: Please check your API key and ensure it is valid. Verify that the API key has been correctly set in your configuration. If using environment variables, ensure MERIT_API_KEY or the service-specific API key variable (e.g., OPENAI_API_KEY) is correctly set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Exception raised: MeritAPIAuthenticationError - Good for debugging!\n",
      "üü¢ Graceful Mode (Production/Evaluation):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed (graceful mode): Authentication failed (Error Code: MAPI-001) \n",
      "\n",
      "Troubleshooting: Please check your API key and ensure it is valid. Verify that the API key has been correctly set in your configuration. If using environment variables, ensure MERIT_API_KEY or the service-specific API key variable (e.g., OPENAI_API_KEY) is correctly set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Returned None/empty - Good for continued processing!\n"
     ]
    }
   ],
   "source": [
    "# Scenario 1: Invalid API key with strict mode\n",
    "print('=== Scenario 1: Invalid API Key (Strict Mode) ===')\n",
    "try:\n",
    "    invalid_client = OpenAIClient(\n",
    "        api_key='invalid-key-12345',\n",
    "        strict=True  # Strict mode - will raise exceptions\n",
    "    )\n",
    "    result = invalid_client.generate_text('Test')\n",
    "    print(f'‚ùå Unexpected success: {result}')\n",
    "except Exception as e:\n",
    "    print(f'‚úÖ MERIT properly handled invalid API key: {type(e).__name__}')\n",
    "\n",
    "# Scenario 1b: Invalid API key with graceful mode\n",
    "print('\\n=== Scenario 1b: Invalid API Key (Graceful Mode) ===')\n",
    "invalid_graceful_client = OpenAIClient(\n",
    "    api_key='invalid-key-12345',\n",
    "    strict=False  # Graceful mode - returns None\n",
    ")\n",
    "result = invalid_graceful_client.generate_text('Test')\n",
    "if result is None:\n",
    "    print('‚úÖ MERIT gracefully handled invalid API key: returned None/empty')\n",
    "else:\n",
    "    print(f'‚ùå Unexpected success: {result}')\n",
    "\n",
    "# Scenario 2: Invalid model with strict mode\n",
    "print('\\n=== Scenario 2: Invalid Model (Strict Mode) ===')\n",
    "try:\n",
    "    invalid_model_client = OpenAIClient(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        model='invalid-model-12345',\n",
    "        strict=True  # Strict mode - will raise exceptions\n",
    "    )\n",
    "    result = invalid_model_client.generate_text('Test')\n",
    "    print(f'‚ùå Unexpected success: {result}')\n",
    "except Exception as e:\n",
    "    print(f'‚úÖ MERIT properly handled invalid model: {type(e).__name__}')\n",
    "\n",
    "# Scenario 3: Graceful degradation with proper error checking\n",
    "print('\\n=== Scenario 3: Graceful Degradation ===')\n",
    "\n",
    "def resilient_app(prompts):\n",
    "    \"\"\"App that continues working even when some calls fail\"\"\"\n",
    "    # Use graceful mode for resilient processing\n",
    "    valid_client = OpenAIClient(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        strict=False  # Graceful mode\n",
    "    )\n",
    "    \n",
    "    # Also test with invalid client for demonstration\n",
    "    invalid_client = OpenAIClient(\n",
    "        api_key='invalid-key-12345',\n",
    "        strict=False  # Graceful mode\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        # Try with valid client\n",
    "        result = valid_client.generate_text(prompt)\n",
    "        if result and len(result.strip()) > 0:\n",
    "            results.append({'prompt': prompt, 'result': result, 'status': 'success'})\n",
    "        else:\n",
    "            # Try with invalid client to show graceful handling\n",
    "            result = invalid_client.generate_text(prompt)\n",
    "            if result and len(result.strip()) > 0:\n",
    "                results.append({'prompt': prompt, 'result': result, 'status': 'success'})\n",
    "            else:\n",
    "                results.append({'prompt': prompt, 'error': 'API call failed gracefully', 'status': 'failed'})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with mix of valid prompts\n",
    "test_prompts = [\n",
    "    'What is Python?',\n",
    "    'Explain machine learning',\n",
    "    'What is the future of AI?'\n",
    "]\n",
    "\n",
    "results = resilient_app(test_prompts)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    if result['status'] == 'success':\n",
    "        print(f'‚úÖ Prompt {i+1}: {result[\"result\"][:50]}...')\n",
    "    else:\n",
    "        print(f'‚ùå Prompt {i+1}: {result[\"error\"]}')\n",
    "\n",
    "success_rate = len([r for r in results if r['status'] == 'success']) / len(results) * 100\n",
    "print(f'\\nüìä Success rate: {success_rate:.1f}%')\n",
    "\n",
    "# Scenario 4: Demonstrating the difference\n",
    "print('\\n=== Scenario 4: Strict vs Graceful Comparison ===')\n",
    "\n",
    "# Strict mode example\n",
    "print('üî¥ Strict Mode (Development/Debugging):')\n",
    "strict_client = OpenAIClient(api_key='invalid-key', strict=True)\n",
    "try:\n",
    "    result = strict_client.generate_text('Test')\n",
    "    print('‚ùå Should have failed')\n",
    "except Exception as e:\n",
    "    print(f'‚úÖ Exception raised: {type(e).__name__} - Good for debugging!')\n",
    "\n",
    "# Graceful mode example\n",
    "print('üü¢ Graceful Mode (Production/Evaluation):')\n",
    "graceful_client = OpenAIClient(api_key='invalid-key', strict=False)\n",
    "result = graceful_client.generate_text('Test')\n",
    "if result is None or result == '':\n",
    "    print('‚úÖ Returned None/empty - Good for continued processing!')\n",
    "else:\n",
    "    print(f'‚ùå Unexpected result: {result}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Error Handling Benefits\n",
    "\n",
    "- **Automatic retries** - Handles transient failures without code changes\n",
    "- **Rate limiting protection** - Prevents 429 errors with adaptive delays\n",
    "- **Adaptive throttling** - Learns optimal request timing automatically\n",
    "- **Graceful degradation** - Applications continue working when possible\n",
    "- **Consistent behavior** - Same error handling across all API calls\n",
    "- **Production ready** - Battle-tested error handling patterns\n",
    "- **Zero configuration** - Works out of the box with sensible defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Caching & Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùå WITHOUT MERIT: No Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå WITHOUT CACHING:\n",
      "Call 1: 0.97s - Got 1536 dimensions\n",
      "Call 2: 0.97s - Got 1536 dimensions\n",
      "Call 3: 1.14s - Got 1536 dimensions\n",
      "Total time: 3.08s\n",
      "Cost: 3 API calls = 3x the cost\n",
      "Average call time: 1.03s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def get_embedding_without_cache(text):\n",
    "    \"\"\"Every call hits the API - expensive and slow\"\"\"\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    response = requests.post(\n",
    "        'https://api.openai.com/v1/embeddings',\n",
    "        headers={'Authorization': f'Bearer {api_key}'},\n",
    "        json={'input': text, 'model': 'text-embedding-ada-002'}\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()['data'][0]['embedding']\n",
    "\n",
    "# Simulate repeated calls with same text\n",
    "text = \"What is machine learning?\"\n",
    "\n",
    "print(\"‚ùå WITHOUT CACHING:\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Each call takes ~500ms and costs money\n",
    "embeddings_no_cache = []\n",
    "for i in range(3):\n",
    "    call_start = time.time()\n",
    "    embedding = get_embedding_without_cache(text)\n",
    "    call_time = time.time() - call_start\n",
    "    embeddings_no_cache.append(embedding)\n",
    "    print(f\"Call {i+1}: {call_time:.2f}s - Got {len(embedding)} dimensions\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total time: {total_time:.2f}s\")\n",
    "print(f\"Cost: 3 API calls = 3x the cost\")\n",
    "print(f\"Average call time: {total_time/3:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ WITH MERIT: Automatic Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ WITH MERIT CACHING:\n",
      "Call 1 (API): 0.00s - Got 1536 dimensions\n",
      "Call 2 (cached): 0.0000s - Got 1536 dimensions\n",
      "Call 3 (cached): 0.0000s - Got 1536 dimensions\n",
      "\n",
      "Total time: 0.00s\n",
      "Cost: 1 API call instead of 3 = 67% cost savings\n",
      "Speed: ~0x faster for cached calls\n",
      "\n",
      "=== Cache Efficiency with Multiple Texts ===\n",
      "Text 1: 0.0000s (cached) - What is machine learning?...\n",
      "Text 2: 0.0000s (cached) - What is deep learning?...\n",
      "Text 3: 0.0000s (cached) - What is machine learning?...\n",
      "Text 4: 0.0000s (cached) - What is artificial intelligenc...\n",
      "Text 5: 0.0000s (cached) - What is deep learning?...\n",
      "\n",
      "Summary:\n",
      "Total time: 0.00s\n",
      "API calls: 0\n",
      "Cache hits: 5\n",
      "Cache hit rate: 100.0%\n",
      "Cost savings: 100.0%\n"
     ]
    }
   ],
   "source": [
    "from merit.api import OpenAIClient\n",
    "import time\n",
    "\n",
    "client = OpenAIClient(api_key=OPENAI_API_KEY)\n",
    "text = \"What is machine learning?\"\n",
    "\n",
    "print(\"‚úÖ WITH MERIT CACHING:\")\n",
    "start_time = time.time()\n",
    "\n",
    "# First call hits the API\n",
    "call_start = time.time()\n",
    "embedding1 = client.get_embeddings([text])\n",
    "first_call_time = time.time() - call_start\n",
    "print(f\"Call 1 (API): {first_call_time:.2f}s - Got {len(embedding1[0])} dimensions\")\n",
    "\n",
    "# Subsequent calls use cache\n",
    "for i in range(2, 4):\n",
    "    call_start = time.time()\n",
    "    embedding = client.get_embeddings([text])\n",
    "    call_time = time.time() - call_start\n",
    "    print(f\"Call {i} (cached): {call_time:.4f}s - Got {len(embedding[0])} dimensions\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time: {total_time:.2f}s\")\n",
    "print(f\"Cost: 1 API call instead of 3 = 67% cost savings\")\n",
    "print(f\"Speed: ~{first_call_time/0.001:.0f}x faster for cached calls\")\n",
    "\n",
    "# Demonstrate cache persistence across different texts\n",
    "print(\"\\n=== Cache Efficiency with Multiple Texts ===\")\n",
    "texts = [\n",
    "    \"What is machine learning?\",  # Already cached\n",
    "    \"What is deep learning?\",     # New - will hit API\n",
    "    \"What is machine learning?\",  # Cached again\n",
    "    \"What is artificial intelligence?\",  # New - will hit API\n",
    "    \"What is deep learning?\",     # Now cached\n",
    "]\n",
    "\n",
    "api_calls = 0\n",
    "cache_hits = 0\n",
    "total_start = time.time()\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    call_start = time.time()\n",
    "    embedding = client.get_embeddings([text])\n",
    "    call_time = time.time() - call_start\n",
    "    \n",
    "    if call_time > 0.1:  # Likely an API call\n",
    "        api_calls += 1\n",
    "        print(f\"Text {i+1}: {call_time:.2f}s (API) - {text[:30]}...\")\n",
    "    else:  # Likely cached\n",
    "        cache_hits += 1\n",
    "        print(f\"Text {i+1}: {call_time:.4f}s (cached) - {text[:30]}...\")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Total time: {total_time:.2f}s\")\n",
    "print(f\"API calls: {api_calls}\")\n",
    "print(f\"Cache hits: {cache_hits}\")\n",
    "print(f\"Cache hit rate: {cache_hits/(api_calls+cache_hits)*100:.1f}%\")\n",
    "print(f\"Cost savings: {cache_hits/(api_calls+cache_hits)*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Features: Retry & Throttling Decorators\n",
    "\n",
    "MERIT provides advanced decorators for handling rate limiting and retries. Let's test these features:\n",
    "\n",
    "### ‚ùå WITHOUT DECORATORS: Manual Retry Logic\n",
    "\n",
    "First, let's see what manual retry logic looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Manual Retry Implementation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed (strict mode): Authentication failed (Error Code: MAPI-001) \n",
      "\n",
      "Troubleshooting: Please check your API key and ensure it is valid. Verify that the API key has been correctly set in your configuration. If using environment variables, ensure MERIT_API_KEY or the service-specific API key variable (e.g., OPENAI_API_KEY) is correctly set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed, waiting 1.6s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed (strict mode): Authentication failed (Error Code: MAPI-001) \n",
      "\n",
      "Troubleshooting: Please check your API key and ensure it is valid. Verify that the API key has been correctly set in your configuration. If using environment variables, ensure MERIT_API_KEY or the service-specific API key variable (e.g., OPENAI_API_KEY) is correctly set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 2 failed, waiting 2.2s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed (strict mode): Authentication failed (Error Code: MAPI-001) \n",
      "\n",
      "Troubleshooting: Please check your API key and ensure it is valid. Verify that the API key has been correctly set in your configuration. If using environment variables, ensure MERIT_API_KEY or the service-specific API key variable (e.g., OPENAI_API_KEY) is correctly set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed after 3 attempts: Authentication failed (Error Code: MAPI-001) \n",
      "\n",
      "Troubleshooting: Please check your API key and ensure it is valid. Verify that the API key has been correctly set in your configuration. If using environment variables, ensure MERIT_API_KEY or the service-specific API key variable (e.g., OPENAI_API_KEY) is correctly set.\n",
      "Final failure: MeritAPIAuthenticationError\n"
     ]
    }
   ],
   "source": [
    "# Manual retry implementation - complex and error-prone\n",
    "import time\n",
    "import random\n",
    "from merit.api import OpenAIClient\n",
    "\n",
    "def manual_retry_api_call(prompt, max_retries=3):\n",
    "    \"\"\"Manual implementation of retries - lots of boilerplate\"\"\"\n",
    "    \n",
    "    client = OpenAIClient(api_key='invalid-key', strict=True)  # Use invalid key to trigger errors\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = client.generate_text(prompt)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Failed after {max_retries} attempts: {e}\")\n",
    "                raise\n",
    "            \n",
    "            # Manual backoff calculation\n",
    "            wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "            print(f\"Attempt {attempt + 1} failed, waiting {wait_time:.1f}s...\")\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test manual retry\n",
    "print(\"=== Manual Retry Implementation ===\")\n",
    "try:\n",
    "    result = manual_retry_api_call(\"Test prompt\")\n",
    "    print(f\"Success: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Final failure: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ WITH MERIT DECORATORS: Automatic Retry & Throttling\n",
    "\n",
    "Now let's use MERIT's decorators:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Basic Retry Decorator ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed (strict mode): Authentication failed (Error Code: MAPI-001) \n",
      "\n",
      "Troubleshooting: Please check your API key and ensure it is valid. Verify that the API key has been correctly set in your configuration. If using environment variables, ensure MERIT_API_KEY or the service-specific API key variable (e.g., OPENAI_API_KEY) is correctly set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Final failure after retries: MeritAPIAuthenticationError\n",
      "\n",
      "=== Test 2: Adaptive Throttling ===\n",
      "Making 5 throttled calls...\n",
      "Making API call #1\n",
      "Call #1 completed in 0.105s\n",
      "Got: Response 1\n",
      "Making API call #2\n",
      "Call #2 completed in 0.105s\n",
      "Got: Response 2\n",
      "Making API call #3\n",
      "Call #3 completed in 0.101s\n",
      "Got: Response 3\n",
      "Making API call #4\n",
      "Call #4 completed in 0.106s\n",
      "Got: Response 4\n",
      "Making API call #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed with MeritAPIServerError, retrying in 0.62 seconds (retry 1/2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call #5 completed in 0.105s\n",
      "Got: Response 5\n",
      "\n",
      "=== Test 3: Combined Adaptive Retry ===\n",
      "Testing combined retry + throttling (may take a moment)...\n",
      "‚úÖ Attempt 1: Success!\n",
      "‚úÖ Attempt 2: Success!\n",
      "‚úÖ Attempt 3: Success!\n"
     ]
    }
   ],
   "source": [
    "# Import the decorators\n",
    "from merit.api.run_config import adaptive_throttle, with_retry, with_adaptive_retry\n",
    "from merit.api import OpenAIClient\n",
    "import time\n",
    "\n",
    "# Test 1: Basic retry decorator\n",
    "print(\"=== Test 1: Basic Retry Decorator ===\")\n",
    "\n",
    "@with_retry(max_retries=3, backoff_factor=0.5)\n",
    "def test_basic_retry():\n",
    "    \"\"\"Function with automatic retry on failures\"\"\"\n",
    "    client = OpenAIClient(api_key='invalid-key', strict=True)\n",
    "    return client.generate_text(\"Test prompt\")\n",
    "\n",
    "try:\n",
    "    result = test_basic_retry()\n",
    "    print(f\"‚úÖ Success: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Final failure after retries: {type(e).__name__}\")\n",
    "\n",
    "# Test 2: Adaptive throttling decorator\n",
    "print(\"\\n=== Test 2: Adaptive Throttling ===\")\n",
    "\n",
    "@adaptive_throttle\n",
    "def test_throttling(call_number):\n",
    "    \"\"\"Function with adaptive rate limiting\"\"\"\n",
    "    print(f\"Making API call #{call_number}\")\n",
    "    # Simulate API call timing\n",
    "    start_time = time.time()\n",
    "    time.sleep(0.1)  # Simulate API response time\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Call #{call_number} completed in {duration:.3f}s\")\n",
    "    return f\"Response {call_number}\"\n",
    "\n",
    "# Make several calls to see adaptive throttling in action\n",
    "print(\"Making 5 throttled calls...\")\n",
    "for i in range(1, 6):\n",
    "    result = test_throttling(i)\n",
    "    print(f\"Got: {result}\")\n",
    "\n",
    "# Test 3: Combined adaptive retry decorator\n",
    "print(\"\\n=== Test 3: Combined Adaptive Retry ===\")\n",
    "\n",
    "@with_adaptive_retry(max_retries=2)\n",
    "def test_combined_features(success_rate=0.3):\n",
    "    \"\"\"Function that randomly fails to test retry + throttling\"\"\"\n",
    "    import random\n",
    "    \n",
    "    if random.random() < success_rate:\n",
    "        return \"Success!\"\n",
    "    else:\n",
    "        # Simulate different types of failures\n",
    "        failure_type = random.choice(['rate_limit', 'connection', 'server'])\n",
    "        if failure_type == 'rate_limit':\n",
    "            from merit.api.errors import MeritAPIRateLimitError\n",
    "            raise MeritAPIRateLimitError(\"Simulated rate limit\")\n",
    "        elif failure_type == 'connection':\n",
    "            from merit.api.errors import MeritAPIConnectionError\n",
    "            raise MeritAPIConnectionError(\"Simulated connection error\")\n",
    "        else:\n",
    "            from merit.api.errors import MeritAPIServerError\n",
    "            raise MeritAPIServerError(\"Simulated server error\")\n",
    "\n",
    "# Test the combined decorator\n",
    "print(\"Testing combined retry + throttling (may take a moment)...\")\n",
    "for i in range(3):\n",
    "    try:\n",
    "        result = test_combined_features(success_rate=0.7)  # 70% success rate\n",
    "        print(f\"‚úÖ Attempt {i+1}: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Attempt {i+1} failed: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Real-World Usage: Decorating API Wrapper Functions\n",
    "\n",
    "Here's how you'd use these decorators in practice:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Real-World Usage Example ===\n",
      "Provider: openai\n",
      "Result: Artificial intelligence (AI) is the simulation of human intelligence processes by machines, especial...\n"
     ]
    }
   ],
   "source": [
    "# Real-world example: Robust API wrapper functions\n",
    "from merit.api import OpenAIClient, GeminiClient\n",
    "from merit.api.run_config import with_adaptive_retry\n",
    "\n",
    "class RobustAPIService:\n",
    "    \"\"\"Example service using MERIT decorators for reliability\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.openai_client = OpenAIClient(strict=False)  # Graceful mode\n",
    "        self.gemini_client = GeminiClient(strict=False)\n",
    "    \n",
    "    @with_adaptive_retry(max_retries=3)\n",
    "    def generate_with_openai(self, prompt):\n",
    "        \"\"\"OpenAI generation with automatic retry and throttling\"\"\"\n",
    "        return self.openai_client.generate_text(prompt)\n",
    "    \n",
    "    @with_adaptive_retry(max_retries=3)\n",
    "    def generate_with_gemini(self, prompt):\n",
    "        \"\"\"Gemini generation with automatic retry and throttling\"\"\"\n",
    "        return self.gemini_client.generate_text(prompt)\n",
    "    \n",
    "    def robust_generation(self, prompt):\n",
    "        \"\"\"Try multiple providers with automatic fallback\"\"\"\n",
    "        # Try OpenAI first\n",
    "        result = self.generate_with_openai(prompt)\n",
    "        if result and len(result.strip()) > 0:\n",
    "            return {\"provider\": \"openai\", \"result\": result}\n",
    "        \n",
    "        # Fallback to Gemini\n",
    "        result = self.generate_with_gemini(prompt)\n",
    "        if result and len(result.strip()) > 0:\n",
    "            return {\"provider\": \"gemini\", \"result\": result}\n",
    "        \n",
    "        return {\"provider\": \"none\", \"result\": \"All providers failed\"}\n",
    "\n",
    "# Test the robust service\n",
    "print(\"\\n=== Real-World Usage Example ===\")\n",
    "service = RobustAPIService()\n",
    "\n",
    "# Test with a simple prompt\n",
    "test_prompt = \"What is artificial intelligence?\"\n",
    "result = service.robust_generation(test_prompt)\n",
    "print(f\"Provider: {result['provider']}\")\n",
    "print(f\"Result: {result['result'][:100]}...\" if result['result'] else \"No result\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed with MeritAPIRateLimitError, retrying in 0.32 seconds (retry 1/3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ MERIT DECORATORS DEMONSTRATION\n",
      "==================================================\n",
      "\n",
      "1Ô∏è‚É£ RETRY DECORATOR - Watch the retries happen:\n",
      "---------------------------------------------\n",
      "   üîÑ Attempting API call #1...\n",
      "   ‚úÖ Success for call #1!\n",
      "   üéâ Final result: Response from call #1\n",
      "\n",
      "   üîÑ Attempting API call #2...\n",
      "   ‚ùå Rate limit hit for call #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed with MeritAPIRateLimitError, retrying in 0.73 seconds (retry 2/3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üîÑ Attempting API call #2...\n",
      "   ‚ùå Rate limit hit for call #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed with MeritAPIRateLimitError, retrying in 1.20 seconds (retry 3/3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üîÑ Attempting API call #2...\n",
      "   ‚ùå Rate limit hit for call #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed with MeritAPIRateLimitError, retrying in 0.32 seconds (retry 1/3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üîÑ Attempting API call #2...\n",
      "   ‚ùå Rate limit hit for call #2\n",
      "   üí• Final failure: MeritAPIRateLimitError\n",
      "\n",
      "   üîÑ Attempting API call #3...\n",
      "   ‚ùå Rate limit hit for call #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed with MeritAPIConnectionError, retrying in 0.60 seconds (retry 2/3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üîÑ Attempting API call #3...\n",
      "   ‚ùå Connection failed for call #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed with MeritAPIConnectionError, retrying in 1.36 seconds (retry 3/3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üîÑ Attempting API call #3...\n",
      "   ‚ùå Connection failed for call #3\n",
      "   üîÑ Attempting API call #3...\n",
      "   ‚ùå Connection failed for call #3\n",
      "   üí• Final failure: MeritAPIConnectionError\n",
      "\n",
      "2Ô∏è‚É£ ADAPTIVE THROTTLING - Watch delays adjust:\n",
      "---------------------------------------------\n",
      "Making 6 throttled calls (watch the delays):\n",
      "   üì° Call #1 completed in 0.051s\n",
      "   ‚úÖ Success #1 (total time: 0.154s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit hit! Increasing delay: 0.079s ‚Üí 0.119s (failure #4)\n",
      "Rate limit error in throttled_api_call: Rate limit - delay will increase (Error Code: MAPI-002) \n",
      "\n",
      "Troubleshooting: The API provider's rate limit has been reached. Please retry after some time. Consider implementing request batching or increasing the delay between requests. If this error persists, you may need to upgrade your API plan for higher rate limits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üì° Call #2 completed in 0.054s\n",
      "   ‚úÖ Success #2 (total time: 0.144s)\n",
      "   üì° Call #3 completed in 0.053s\n",
      "   ‚ö†Ô∏è  Simulating rate limit on call #3\n",
      "   ‚ùå MeritAPIRateLimitError - delay will increase\n",
      "   üì° Call #4 completed in 0.055s\n",
      "   ‚úÖ Success #4 (total time: 0.179s)\n",
      "   üì° Call #5 completed in 0.054s\n",
      "   ‚úÖ Success #5 (total time: 0.166s)\n",
      "   üì° Call #6 completed in 0.056s\n",
      "   ‚úÖ Success #6 (total time: 0.157s)\n",
      "\n",
      "3Ô∏è‚É£ COMBINED ADAPTIVE RETRY - Retries + Throttling:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed with MeritAPIRateLimitError, retrying in 0.60 seconds (retry 1/2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üöÄ Processing 'Data Analysis' (attempt #1)\n",
      "   ‚è≥ Rate limited - will retry with increased delay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed with MeritAPIRateLimitError, retrying in 1.05 seconds (retry 2/2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üöÄ Processing 'Data Analysis' (attempt #2)\n",
      "   ‚è≥ Rate limited - will retry with increased delay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit hit! Increasing delay: 0.323s ‚Üí 0.484s (failure #2)\n",
      "Rate limit error in robust_api_call: Rate limit exceeded (Error Code: MAPI-002) \n",
      "\n",
      "Troubleshooting: The API provider's rate limit has been reached. Please retry after some time. Consider implementing request batching or increasing the delay between requests. If this error persists, you may need to upgrade your API plan for higher rate limits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üöÄ Processing 'Data Analysis' (attempt #3)\n",
      "   ‚è≥ Rate limited - will retry with increased delay\n",
      "   üí• Task 'Data Analysis' failed after retries: MeritAPIRateLimitError\n",
      "\n",
      "   üöÄ Processing 'Report Generation' (attempt #1)\n",
      "   üéØ Successfully processed 'Report Generation'\n",
      "   ‚úÖ Completed: Report Generation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed with MeritAPIConnectionError, retrying in 0.58 seconds (retry 1/2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üöÄ Processing 'Model Training' (attempt #1)\n",
      "   üîå Connection error - will retry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "API call failed with MeritAPIRateLimitError, retrying in 1.24 seconds (retry 2/2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üöÄ Processing 'Model Training' (attempt #2)\n",
      "   ‚è≥ Rate limited - will retry with increased delay\n",
      "   üöÄ Processing 'Model Training' (attempt #3)\n",
      "   üéØ Successfully processed 'Model Training'\n",
      "   ‚úÖ Completed: Model Training\n",
      "\n",
      "üéØ SUMMARY:\n",
      "‚úÖ Retry decorator: Automatically retries failed calls with exponential backoff\n",
      "‚úÖ Adaptive throttling: Learns optimal timing and adjusts delays dynamically\n",
      "‚úÖ Combined approach: Production-ready resilience for unreliable APIs\n",
      "\n",
      "üí° These decorators can be applied to any function that makes API calls!\n"
     ]
    }
   ],
   "source": [
    "# MERIT Decorators Demo: Retries + Throttling in Action\n",
    "import time\n",
    "import random\n",
    "from merit.api.run_config import with_adaptive_retry, adaptive_throttle, with_retry\n",
    "from merit.api.errors import MeritAPIRateLimitError, MeritAPIConnectionError\n",
    "\n",
    "print(\"üî¨ MERIT DECORATORS DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Demo 1: Retry Decorator - Shows actual retry attempts\n",
    "print(\"\\n1Ô∏è‚É£ RETRY DECORATOR - Watch the retries happen:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "@with_retry(max_retries=3, backoff_factor=0.3)\n",
    "def flaky_api_call(call_id):\n",
    "    \"\"\"Simulates an API that fails 70% of the time\"\"\"\n",
    "    print(f\"   üîÑ Attempting API call #{call_id}...\")\n",
    "    \n",
    "    if random.random() < 0.7:  # 70% failure rate\n",
    "        error_type = random.choice(['rate_limit', 'connection'])\n",
    "        if error_type == 'rate_limit':\n",
    "            print(f\"   ‚ùå Rate limit hit for call #{call_id}\")\n",
    "            raise MeritAPIRateLimitError(\"API rate limit exceeded\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Connection failed for call #{call_id}\")\n",
    "            raise MeritAPIConnectionError(\"Connection timeout\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Success for call #{call_id}!\")\n",
    "        return f\"Response from call #{call_id}\"\n",
    "\n",
    "# Test the retry decorator\n",
    "for i in range(1, 4):\n",
    "    try:\n",
    "        result = flaky_api_call(i)\n",
    "        print(f\"   üéâ Final result: {result}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   üí• Final failure: {type(e).__name__}\\n\")\n",
    "\n",
    "# Demo 2: Adaptive Throttling - Shows delay adjustments\n",
    "print(\"2Ô∏è‚É£ ADAPTIVE THROTTLING - Watch delays adjust:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "call_count = 0\n",
    "\n",
    "@adaptive_throttle\n",
    "def throttled_api_call():\n",
    "    \"\"\"Shows adaptive throttling in action\"\"\"\n",
    "    global call_count\n",
    "    call_count += 1\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Simulate API processing time\n",
    "    time.sleep(0.05)  # 50ms API response time\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"   üì° Call #{call_count} completed in {duration:.3f}s\")\n",
    "    \n",
    "    # Simulate occasional rate limiting to show adaptation\n",
    "    if call_count == 3:\n",
    "        print(f\"   ‚ö†Ô∏è  Simulating rate limit on call #{call_count}\")\n",
    "        raise MeritAPIRateLimitError(\"Rate limit - delay will increase\")\n",
    "    \n",
    "    return f\"Success #{call_count}\"\n",
    "\n",
    "# Make several calls to see throttling adapt\n",
    "print(\"Making 6 throttled calls (watch the delays):\")\n",
    "for i in range(6):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        result = throttled_api_call()\n",
    "        total_time = time.time() - start\n",
    "        print(f\"   ‚úÖ {result} (total time: {total_time:.3f}s)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå {type(e).__name__} - delay will increase\")\n",
    "\n",
    "# Demo 3: Combined Adaptive Retry - Shows both features together\n",
    "print(\"\\n3Ô∏è‚É£ COMBINED ADAPTIVE RETRY - Retries + Throttling:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "attempt_count = 0\n",
    "\n",
    "@with_adaptive_retry(max_retries=2)\n",
    "def robust_api_call(task_name):\n",
    "    \"\"\"Shows combined retry + throttling\"\"\"\n",
    "    global attempt_count\n",
    "    attempt_count += 1\n",
    "    \n",
    "    print(f\"   üöÄ Processing '{task_name}' (attempt #{attempt_count})\")\n",
    "    \n",
    "    # Simulate different failure scenarios\n",
    "    failure_chance = random.random()\n",
    "    \n",
    "    if failure_chance < 0.4:  # 40% chance of rate limit\n",
    "        print(f\"   ‚è≥ Rate limited - will retry with increased delay\")\n",
    "        raise MeritAPIRateLimitError(\"Rate limit exceeded\")\n",
    "    elif failure_chance < 0.6:  # 20% chance of connection error\n",
    "        print(f\"   üîå Connection error - will retry\")\n",
    "        raise MeritAPIConnectionError(\"Connection failed\")\n",
    "    else:  # 40% chance of success\n",
    "        print(f\"   üéØ Successfully processed '{task_name}'\")\n",
    "        return f\"Completed: {task_name}\"\n",
    "\n",
    "# Test combined features\n",
    "tasks = [\"Data Analysis\", \"Report Generation\", \"Model Training\"]\n",
    "\n",
    "for task in tasks:\n",
    "    attempt_count = 0  # Reset for each task\n",
    "    try:\n",
    "        result = robust_api_call(task)\n",
    "        print(f\"   ‚úÖ {result}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"   üí• Task '{task}' failed after retries: {type(e).__name__}\\n\")\n",
    "\n",
    "print(\"üéØ SUMMARY:\")\n",
    "print(\"‚úÖ Retry decorator: Automatically retries failed calls with exponential backoff\")\n",
    "print(\"‚úÖ Adaptive throttling: Learns optimal timing and adjusts delays dynamically\") \n",
    "print(\"‚úÖ Combined approach: Production-ready resilience for unreliable APIs\")\n",
    "print(\"\\nüí° These decorators can be applied to any function that makes API calls!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Decorator Benefits Summary\n",
    "\n",
    " ‚úÖ **AUTOMATIC RETRY LOGIC:**\n",
    "- Handles transient failures automatically\n",
    "- Exponential backoff with jitter\n",
    "- Configurable retry attempts\n",
    "\n",
    " ‚úÖ **ADAPTIVE RATE LIMITING:**\n",
    "- Learns optimal request timing\n",
    "- Prevents 429 rate limit errors\n",
    "- Reduces delay after successful calls\n",
    "\n",
    " ‚úÖ **PRODUCTION READY:**\n",
    "- Thread-safe implementation\n",
    "- Comprehensive error handling\n",
    "- Detailed logging and statistics\n",
    "\n",
    " ‚úÖ **EASY TO USE:**\n",
    "- Simple decorator syntax\n",
    "- Works with any function\n",
    "- Configurable parameters\n",
    "\n",
    " üî• **PERFECT FOR:**\n",
    "- High-volume API usage\n",
    "- Production applications\n",
    "- Unreliable network conditions\n",
    "- Cost-sensitive operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## . Next Steps\n",
    "\n",
    "Now that you've seen how MERIT simplifies LLM API usage, here are some next steps:\n",
    "\n",
    "### üîó Learn More\n",
    "- Check out the full MERIT documentation\n",
    "- Explore other MERIT modules (evaluation, monitoring, etc.)\n",
    "- Join the MERIT community\n",
    "\n",
    "### üöÄ Build Something\n",
    "- Create your own RAG system\n",
    "- Build a multi-model chatbot\n",
    "- Implement cost-effective batch processing\n",
    "\n",
    "### üí° Contribute\n",
    "- Report issues or suggest features\n",
    "- Contribute to the open-source project\n",
    "- Share your MERIT success stories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
